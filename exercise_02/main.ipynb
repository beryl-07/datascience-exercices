{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercice 2: _Extraction de DonnÃ©es en Ligne_\n",
    "**Objectif:** Ecrivons les scripts d'extraction de donnÃ©es depuis diffÃ©rentes plateformes."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:40:25.704532Z",
     "start_time": "2025-04-01T18:40:25.359558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urllib.request import urlopen\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googlesearch import search\n",
    "import praw\n",
    "import wikipediaapi\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgooglesearch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m search\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpraw\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mwikipediaapi\u001B[39;00m\n\u001B[1;32m     11\u001B[0m config \u001B[38;5;241m=\u001B[39m dotenv_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.env\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/data_science/lib/python3.8/site-packages/wikipediaapi/__init__.py:33\u001B[0m\n\u001B[1;32m     29\u001B[0m log \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# https://www.mediawiki.org/wiki/API:Main_page\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m PagesDict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWikipediaPage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mExtractFormat\u001B[39;00m(IntEnum):\n\u001B[1;32m     37\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Represents extraction format.\"\"\"\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. **Amazon:** Web scraping avec BeautifulSoup: Produits, prix, avis",
   "id": "4ed4429937c283cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.114062218Z",
     "start_time": "2025-04-01T18:30:44.315684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://www.amazon.com/dp/B0CP22DQQS?th=1\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ],
   "id": "555d354fdf7d4fcd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.145296608Z",
     "start_time": "2025-04-01T18:30:49.629913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "title = soup.find(id=\"productTitle\").get_text(strip=True)\n",
    "price = soup.find('span', {'class': 'a-price'}).get_text(strip=True).split('$')[1]\n",
    "rating = soup.find(class_=\"a-icon-alt\").get_text()\n",
    "print(f\"Produit: {title}\\nPrix: {price}$\\nAvis: {rating}\")"
   ],
   "id": "9e03dc510d3c025f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produit: Marsail Ergonomic Office Chair: Office Computer Desk Chair with High Back Mesh and Adjustable Lumbar Support Rolling Work Swivel Task Chairs with Wheel 3D Armrests and Headrest\n",
      "Prix: 107.78$\n",
      "Avis: 4.4 out of 5 stars\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 2. **Twitter:** Utilisation de l'API Twitter v2: Tweets, likes, retweets",
   "id": "8f7afba164c3e3ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.146800131Z",
     "start_time": "2025-04-01T18:30:52.119443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_url=\"https://api.twitter.com/2\"\n",
    "\n",
    "X_BEARER_TOKEN = config.get(\"X_BEARER_TOKEN\")\n",
    "\n",
    "headers = {\n",
    "  'Authorization': f\"Bearer {X_BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'tweet.fields': 'created_at,public_metrics',\n",
    "}\n",
    "\n",
    "if not X_BEARER_TOKEN:\n",
    "   raise Exception(\"Bearer token non trouvÃ©. Merci de fournir un valeur Ã  la variable d'environnement X_BEARER_TOKEN.\")\n",
    "\n",
    "def get_tweets(username: str):\n",
    "    response = get(f\"{x_url}/tweets/search/recent?query=from:{username}\", headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "    return data.get(\"data\")\n",
    "\n",
    "df = pd.DataFrame(get_tweets(\"DylanCalluy\"))\n",
    "print(df.head())"
   ],
   "id": "d267db7405888f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 created_at edit_history_tweet_ids  \\\n",
      "0  2025-04-01T18:21:19.000Z  [1907136208083316864]   \n",
      "1  2025-04-01T17:13:36.000Z  [1907119167032959004]   \n",
      "2  2025-04-01T17:09:16.000Z  [1907118077122097595]   \n",
      "3  2025-04-01T13:13:51.000Z  [1907058833001722033]   \n",
      "4  2025-04-01T09:38:04.000Z  [1907004527099670577]   \n",
      "\n",
      "                                      public_metrics  \\\n",
      "0  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "1  {'retweet_count': 0, 'reply_count': 1, 'like_c...   \n",
      "2  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "3  {'retweet_count': 10, 'reply_count': 1, 'like_...   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                text                   id  \n",
      "0                       @ryanadrift Itâ€™s crying haha  1907136208083316864  \n",
      "1  I wish that I had this reach split across all ...  1907119167032959004  \n",
      "2                           @Farjads_Shots Thanks ðŸ™ðŸ½  1907118077122097595  \n",
      "3  Early mornings in Tokyo ðŸ‡¯ðŸ‡µ https://t.co/rL7CB8...  1907058833001722033  \n",
      "4        @visacashapprb @bytegan Where I wanna be :)  1907004527099670577  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 3. **Instagram:** API Instagram Graph: Captions, likes, images",
   "id": "38d8638768c7808e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.148048273Z",
     "start_time": "2025-03-27T18:56:24.030128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INSTAGRAM_ACCESS_TOKEN = config.get(\"INSTAGRAM_ACCESS_TOKEN\")\n",
    "if not INSTAGRAM_ACCESS_TOKEN:\n",
    "    raise Exception(\"Access token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN.\")\n",
    "\n",
    "def get_instagram_posts(user_id, limit=10):\n",
    "    url = f\"https://graph.instagram.com/{user_id}/media\"\n",
    "    params = {\n",
    "        'fields': 'id,caption,media_url,like_count',\n",
    "        'access_token': INSTAGRAM_ACCESS_TOKEN,\n",
    "        'limit': limit\n",
    "    }\n",
    "    response = get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "    posts = []\n",
    "    for post in data['data']:\n",
    "        post_data = {\n",
    "            'caption': post.get('caption'),\n",
    "            'media_url': post.get('media_url'),\n",
    "            'like_count': post.get('like_count')\n",
    "        }\n",
    "        posts.append(post_data)\n",
    "    return posts\n",
    "\n",
    "user_id = \"1090336554\"\n",
    "posts = get_instagram_posts(user_id)\n",
    "df = pd.DataFrame(posts)\n",
    "print(df.head())"
   ],
   "id": "209dd8387678c28f",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Access token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m INSTAGRAM_ACCESS_TOKEN \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINSTAGRAM_ACCESS_TOKEN\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m INSTAGRAM_ACCESS_TOKEN:\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccess token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_instagram_posts\u001B[39m(user_id, limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m      6\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://graph.instagram.com/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/media\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mException\u001B[0m: Access token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. **YouTube:** API YouTube Data: Titres, vues, commentaires",
   "id": "b3e337564432dc6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.149026602Z",
     "start_time": "2025-04-01T18:31:02.176218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "YOUTUBE_API_KEY = config.get(\"YOUTUBE_API_KEY\")\n",
    "if not YOUTUBE_API_KEY:\n",
    "   raise Exception(\"API Key non trouvÃ©. Merci de fournir un valeur Ã  la variable d'environnement YOUTUBE_API_KEY.\")\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "def get_videos(channel_id: str):\n",
    "    request = youtube.search().list(part=\"snippet\", channelId=channel_id, maxResults=10)\n",
    "    response = request.execute()\n",
    "    videos = []\n",
    "    for item in response['items']:\n",
    "        if item['id']['kind'] == 'youtube#video':\n",
    "            video_id = item['id']['videoId']\n",
    "            video_details = youtube.videos().list(part=\"snippet,statistics\", id=video_id).execute()\n",
    "            video_info = video_details['items'][0]\n",
    "            comments_request = youtube.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=10)\n",
    "            comments_response = comments_request.execute()\n",
    "            comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay'] for comment in comments_response['items']]\n",
    "            video_data = {\n",
    "                'title': video_info['snippet']['title'],\n",
    "                'views': video_info['statistics']['viewCount'],\n",
    "                'comments': comments\n",
    "            }\n",
    "            videos.append(video_data)\n",
    "    return videos\n",
    "\n",
    "videos = get_videos(\"UCWedHS9qKebauVIK2J7383g\")\n",
    "df = pd.DataFrame(videos)\n",
    "print(df.head())"
   ],
   "id": "255eaae2102bcc23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title    views  \\\n",
      "0  VidÃ©o complÃ¨te sur la chaÃ®ne ! Backstage de no...    13883   \n",
      "1  VidÃ©o complÃ¨te sur la chaÃ®ne ! Q&A exclusive :...  1709213   \n",
      "2            Le rÃ´le abject dâ€™un flic sur le Darkweb   825636   \n",
      "3  Le rÃ´le abject d'un garde d'Obama sur le dark web   417989   \n",
      "4  La trouvaille scandaleuse d'un hacker sur un d...   383774   \n",
      "\n",
      "                                            comments  \n",
      "0  [Le futur hugo dÃ©crypte ? ðŸ˜‰ðŸ˜‰, il a un appareil...  \n",
      "1  [Je trouve qu il lui fait des yeux douxðŸ˜‚, On e...  \n",
      "2  [Christophe coulons, Il a dÃ©crochÃ© le job car ...  \n",
      "3  [C&#39;est dÃ©bile..<br><br>Pourquoi le mec est...  \n",
      "4  [Depuis quand tu prends autant de drogue dâ€™un ...  \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 5. **Google Search:** Scraper avec googlesearch (RÃ©sultats de recherche)",
   "id": "3c53c595a8e02841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.149653279Z",
     "start_time": "2025-04-01T18:31:13.856809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def google_search(query, num_results=10):\n",
    "    search_results = []\n",
    "    for result in search(query, num_results=num_results):\n",
    "        search_results.append(result)\n",
    "    return search_results\n",
    "\n",
    "query = 'Data Science Course site:*.edu filetype:pdf intext:\"Book\"'\n",
    "results = google_search(query)\n",
    "df = pd.DataFrame(results)\n",
    "print(df.head())"
   ],
   "id": "71c4bd5e6c92407d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0            https://www.cs.cornell.edu/jeh/book.pdf\n",
      "1  https://www.webpages.uidaho.edu/~stevel/517/Th...\n",
      "2  https://digital.library.ncat.edu/cgi/viewconte...\n",
      "3  https://cims.nyu.edu/~cfgranda/pages/stuff/pro...\n",
      "4  https://www.cs.umd.edu/class/fall2018/cmsc641/...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6. **Reddit:** API Reddit (PRAW). (Posts, votes, commentaires)",
   "id": "124cfe4cab235f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.174877934Z",
     "start_time": "2025-04-01T18:31:18.784844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=config.get(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=config.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=config.get(\"REDDIT_USER_AGENT\")\n",
    ")\n",
    "\n",
    "def get_reddit_posts(subreddit_name, limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    for post in subreddit.hot(limit=limit):\n",
    "        post_data = {\n",
    "            'title': post.title,\n",
    "            'score': post.score,\n",
    "            'num_comments': post.num_comments,\n",
    "            'comments': [comment.body for comment in post.comments[:10]]\n",
    "        }\n",
    "        posts.append(post_data)\n",
    "    return posts\n",
    "\n",
    "subreddit_name = \"learnpython\"\n",
    "posts = get_reddit_posts(subreddit_name)\n",
    "df = pd.DataFrame(posts)\n",
    "print(df.head())"
   ],
   "id": "8d65b5f88332f07d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score  num_comments  \\\n",
      "0                Ask Anything Monday - Weekly Thread      4            11   \n",
      "1  Can I use Qt with Python (PyQt) for a non-comm...      7             7   \n",
      "2               Should I refer to a book or a course      6             6   \n",
      "3  How to dynamically set logging level in this e...      6             2   \n",
      "4         Best file format for external data storage      3             4   \n",
      "\n",
      "                                            comments  \n",
      "0  [Looking for some places to get started. I hav...  \n",
      "1  [The licensing is a bit iffy. I would recommen...  \n",
      "2  [There are only 2 choices.\\n\\nJust choose one....  \n",
      "3  [for starters, you do not need the `log_levels...  \n",
      "4  [All of those options are fine. CSV is probabl...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7. **Wikipedia:** API WikipÃ©dia (Contenu d'articles).",
   "id": "c4cd10cbd0a5a3c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.176638366Z",
     "start_time": "2025-03-27T18:23:10.313783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_wikipedia_content(page_name):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent='data_science_exo')\n",
    "    page = wiki_wiki.page(page_name)\n",
    "    if page.exists():\n",
    "        return {\n",
    "            'title': page.title,\n",
    "            'summary': page.summary,\n",
    "            'full_content': page.text\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "page_name = \"Dunningâ€“Kruger_effect\"\n",
    "content = get_wikipedia_content(page_name)\n",
    "if content:\n",
    "    df = pd.DataFrame([content])\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Page '{page_name}' does not exist.\")"
   ],
   "id": "d38d1f41e03f9bce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   title                                            summary  \\\n",
      "0  Dunningâ€“Kruger effect  The Dunningâ€“Kruger effect is a cognitive bias ...   \n",
      "\n",
      "                                        full_content  \n",
      "0  The Dunningâ€“Kruger effect is a cognitive bias ...  \n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
