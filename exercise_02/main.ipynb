{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercice 2: _Extraction de Données en Ligne_\n",
    "**Objectif:** Ecrivons les scripts d'extraction de données depuis différentes plateformes."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:40:25.704532Z",
     "start_time": "2025-04-01T18:40:25.359558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from urllib.request import urlopen\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import dotenv_values\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from googlesearch import search\n",
    "import praw\n",
    "import wikipediaapi\n",
    "\n",
    "config = dotenv_values(\".env\")"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mgooglesearch\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m search\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpraw\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mwikipediaapi\u001B[39;00m\n\u001B[1;32m     11\u001B[0m config \u001B[38;5;241m=\u001B[39m dotenv_values(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.env\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/data_science/lib/python3.8/site-packages/wikipediaapi/__init__.py:33\u001B[0m\n\u001B[1;32m     29\u001B[0m log \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# https://www.mediawiki.org/wiki/API:Main_page\u001B[39;00m\n\u001B[0;32m---> 33\u001B[0m PagesDict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mdict\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mWikipediaPage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mExtractFormat\u001B[39;00m(IntEnum):\n\u001B[1;32m     37\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Represents extraction format.\"\"\"\u001B[39;00m\n",
      "\u001B[0;31mTypeError\u001B[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. **Amazon:** Web scraping avec BeautifulSoup: Produits, prix, avis",
   "id": "4ed4429937c283cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.114062218Z",
     "start_time": "2025-04-01T18:30:44.315684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://www.amazon.com/dp/B0CP22DQQS?th=1\"\n",
    "html = urlopen(url)\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ],
   "id": "555d354fdf7d4fcd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.145296608Z",
     "start_time": "2025-04-01T18:30:49.629913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "title = soup.find(id=\"productTitle\").get_text(strip=True)\n",
    "price = soup.find('span', {'class': 'a-price'}).get_text(strip=True).split('$')[1]\n",
    "rating = soup.find(class_=\"a-icon-alt\").get_text()\n",
    "print(f\"Produit: {title}\\nPrix: {price}$\\nAvis: {rating}\")"
   ],
   "id": "9e03dc510d3c025f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produit: Marsail Ergonomic Office Chair: Office Computer Desk Chair with High Back Mesh and Adjustable Lumbar Support Rolling Work Swivel Task Chairs with Wheel 3D Armrests and Headrest\n",
      "Prix: 107.78$\n",
      "Avis: 4.4 out of 5 stars\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 2. **Twitter:** Utilisation de l'API Twitter v2: Tweets, likes, retweets",
   "id": "8f7afba164c3e3ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.146800131Z",
     "start_time": "2025-04-01T18:30:52.119443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_url=\"https://api.twitter.com/2\"\n",
    "\n",
    "X_BEARER_TOKEN = config.get(\"X_BEARER_TOKEN\")\n",
    "\n",
    "headers = {\n",
    "  'Authorization': f\"Bearer {X_BEARER_TOKEN}\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'tweet.fields': 'created_at,public_metrics',\n",
    "}\n",
    "\n",
    "if not X_BEARER_TOKEN:\n",
    "   raise Exception(\"Bearer token non trouvé. Merci de fournir un valeur à la variable d'environnement X_BEARER_TOKEN.\")\n",
    "\n",
    "def get_tweets(username: str):\n",
    "    response = get(f\"{x_url}/tweets/search/recent?query=from:{username}\", headers=headers, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Erreur {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "    return data.get(\"data\")\n",
    "\n",
    "df = pd.DataFrame(get_tweets(\"DylanCalluy\"))\n",
    "print(df.head())"
   ],
   "id": "d267db7405888f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 created_at edit_history_tweet_ids  \\\n",
      "0  2025-04-01T18:21:19.000Z  [1907136208083316864]   \n",
      "1  2025-04-01T17:13:36.000Z  [1907119167032959004]   \n",
      "2  2025-04-01T17:09:16.000Z  [1907118077122097595]   \n",
      "3  2025-04-01T13:13:51.000Z  [1907058833001722033]   \n",
      "4  2025-04-01T09:38:04.000Z  [1907004527099670577]   \n",
      "\n",
      "                                      public_metrics  \\\n",
      "0  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "1  {'retweet_count': 0, 'reply_count': 1, 'like_c...   \n",
      "2  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "3  {'retweet_count': 10, 'reply_count': 1, 'like_...   \n",
      "4  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
      "\n",
      "                                                text                   id  \n",
      "0                       @ryanadrift It’s crying haha  1907136208083316864  \n",
      "1  I wish that I had this reach split across all ...  1907119167032959004  \n",
      "2                           @Farjads_Shots Thanks 🙏🏽  1907118077122097595  \n",
      "3  Early mornings in Tokyo 🇯🇵 https://t.co/rL7CB8...  1907058833001722033  \n",
      "4        @visacashapprb @bytegan Where I wanna be :)  1907004527099670577  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 3. **Instagram:** API Instagram Graph: Captions, likes, images",
   "id": "38d8638768c7808e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.148048273Z",
     "start_time": "2025-03-27T18:56:24.030128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INSTAGRAM_ACCESS_TOKEN = config.get(\"INSTAGRAM_ACCESS_TOKEN\")\n",
    "if not INSTAGRAM_ACCESS_TOKEN:\n",
    "    raise Exception(\"Access token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN.\")\n",
    "\n",
    "def get_instagram_posts(user_id, limit=10):\n",
    "    url = f\"https://graph.instagram.com/{user_id}/media\"\n",
    "    params = {\n",
    "        'fields': 'id,caption,media_url,like_count',\n",
    "        'access_token': INSTAGRAM_ACCESS_TOKEN,\n",
    "        'limit': limit\n",
    "    }\n",
    "    response = get(url, params=params)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "    posts = []\n",
    "    for post in data['data']:\n",
    "        post_data = {\n",
    "            'caption': post.get('caption'),\n",
    "            'media_url': post.get('media_url'),\n",
    "            'like_count': post.get('like_count')\n",
    "        }\n",
    "        posts.append(post_data)\n",
    "    return posts\n",
    "\n",
    "user_id = \"1090336554\"\n",
    "posts = get_instagram_posts(user_id)\n",
    "df = pd.DataFrame(posts)\n",
    "print(df.head())"
   ],
   "id": "209dd8387678c28f",
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Access token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m INSTAGRAM_ACCESS_TOKEN \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mINSTAGRAM_ACCESS_TOKEN\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m INSTAGRAM_ACCESS_TOKEN:\n\u001B[0;32m----> 3\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccess token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_instagram_posts\u001B[39m(user_id, limit\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[1;32m      6\u001B[0m     url \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://graph.instagram.com/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00muser_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/media\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mException\u001B[0m: Access token not found. Please provide a value for the environment variable INSTAGRAM_ACCESS_TOKEN."
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. **YouTube:** API YouTube Data: Titres, vues, commentaires",
   "id": "b3e337564432dc6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.149026602Z",
     "start_time": "2025-04-01T18:31:02.176218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "YOUTUBE_API_KEY = config.get(\"YOUTUBE_API_KEY\")\n",
    "if not YOUTUBE_API_KEY:\n",
    "   raise Exception(\"API Key non trouvé. Merci de fournir un valeur à la variable d'environnement YOUTUBE_API_KEY.\")\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)\n",
    "\n",
    "def get_videos(channel_id: str):\n",
    "    request = youtube.search().list(part=\"snippet\", channelId=channel_id, maxResults=10)\n",
    "    response = request.execute()\n",
    "    videos = []\n",
    "    for item in response['items']:\n",
    "        if item['id']['kind'] == 'youtube#video':\n",
    "            video_id = item['id']['videoId']\n",
    "            video_details = youtube.videos().list(part=\"snippet,statistics\", id=video_id).execute()\n",
    "            video_info = video_details['items'][0]\n",
    "            comments_request = youtube.commentThreads().list(part=\"snippet\", videoId=video_id, maxResults=10)\n",
    "            comments_response = comments_request.execute()\n",
    "            comments = [comment['snippet']['topLevelComment']['snippet']['textDisplay'] for comment in comments_response['items']]\n",
    "            video_data = {\n",
    "                'title': video_info['snippet']['title'],\n",
    "                'views': video_info['statistics']['viewCount'],\n",
    "                'comments': comments\n",
    "            }\n",
    "            videos.append(video_data)\n",
    "    return videos\n",
    "\n",
    "videos = get_videos(\"UCWedHS9qKebauVIK2J7383g\")\n",
    "df = pd.DataFrame(videos)\n",
    "print(df.head())"
   ],
   "id": "255eaae2102bcc23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title    views  \\\n",
      "0  Vidéo complète sur la chaîne ! Backstage de no...    13883   \n",
      "1  Vidéo complète sur la chaîne ! Q&A exclusive :...  1709213   \n",
      "2            Le rôle abject d’un flic sur le Darkweb   825636   \n",
      "3  Le rôle abject d'un garde d'Obama sur le dark web   417989   \n",
      "4  La trouvaille scandaleuse d'un hacker sur un d...   383774   \n",
      "\n",
      "                                            comments  \n",
      "0  [Le futur hugo décrypte ? 😉😉, il a un appareil...  \n",
      "1  [Je trouve qu il lui fait des yeux doux😂, On e...  \n",
      "2  [Christophe coulons, Il a décroché le job car ...  \n",
      "3  [C&#39;est débile..<br><br>Pourquoi le mec est...  \n",
      "4  [Depuis quand tu prends autant de drogue d’un ...  \n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " 5. **Google Search:** Scraper avec googlesearch (Résultats de recherche)",
   "id": "3c53c595a8e02841"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.149653279Z",
     "start_time": "2025-04-01T18:31:13.856809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def google_search(query, num_results=10):\n",
    "    search_results = []\n",
    "    for result in search(query, num_results=num_results):\n",
    "        search_results.append(result)\n",
    "    return search_results\n",
    "\n",
    "query = 'Data Science Course site:*.edu filetype:pdf intext:\"Book\"'\n",
    "results = google_search(query)\n",
    "df = pd.DataFrame(results)\n",
    "print(df.head())"
   ],
   "id": "71c4bd5e6c92407d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0\n",
      "0            https://www.cs.cornell.edu/jeh/book.pdf\n",
      "1  https://www.webpages.uidaho.edu/~stevel/517/Th...\n",
      "2  https://digital.library.ncat.edu/cgi/viewconte...\n",
      "3  https://cims.nyu.edu/~cfgranda/pages/stuff/pro...\n",
      "4  https://www.cs.umd.edu/class/fall2018/cmsc641/...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6. **Reddit:** API Reddit (PRAW). (Posts, votes, commentaires)",
   "id": "124cfe4cab235f52"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.174877934Z",
     "start_time": "2025-04-01T18:31:18.784844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=config.get(\"REDDIT_CLIENT_ID\"),\n",
    "    client_secret=config.get(\"REDDIT_CLIENT_SECRET\"),\n",
    "    user_agent=config.get(\"REDDIT_USER_AGENT\")\n",
    ")\n",
    "\n",
    "def get_reddit_posts(subreddit_name, limit=10):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts = []\n",
    "    for post in subreddit.hot(limit=limit):\n",
    "        post_data = {\n",
    "            'title': post.title,\n",
    "            'score': post.score,\n",
    "            'num_comments': post.num_comments,\n",
    "            'comments': [comment.body for comment in post.comments[:10]]\n",
    "        }\n",
    "        posts.append(post_data)\n",
    "    return posts\n",
    "\n",
    "subreddit_name = \"learnpython\"\n",
    "posts = get_reddit_posts(subreddit_name)\n",
    "df = pd.DataFrame(posts)\n",
    "print(df.head())"
   ],
   "id": "8d65b5f88332f07d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score  num_comments  \\\n",
      "0                Ask Anything Monday - Weekly Thread      4            11   \n",
      "1  Can I use Qt with Python (PyQt) for a non-comm...      7             7   \n",
      "2               Should I refer to a book or a course      6             6   \n",
      "3  How to dynamically set logging level in this e...      6             2   \n",
      "4         Best file format for external data storage      3             4   \n",
      "\n",
      "                                            comments  \n",
      "0  [Looking for some places to get started. I hav...  \n",
      "1  [The licensing is a bit iffy. I would recommen...  \n",
      "2  [There are only 2 choices.\\n\\nJust choose one....  \n",
      "3  [for starters, you do not need the `log_levels...  \n",
      "4  [All of those options are fine. CSV is probabl...  \n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7. **Wikipedia:** API Wikipédia (Contenu d'articles).",
   "id": "c4cd10cbd0a5a3c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T18:39:16.176638366Z",
     "start_time": "2025-03-27T18:23:10.313783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_wikipedia_content(page_name):\n",
    "    wiki_wiki = wikipediaapi.Wikipedia(user_agent='data_science_exo')\n",
    "    page = wiki_wiki.page(page_name)\n",
    "    if page.exists():\n",
    "        return {\n",
    "            'title': page.title,\n",
    "            'summary': page.summary,\n",
    "            'full_content': page.text\n",
    "        }\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "page_name = \"Dunning–Kruger_effect\"\n",
    "content = get_wikipedia_content(page_name)\n",
    "if content:\n",
    "    df = pd.DataFrame([content])\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(f\"Page '{page_name}' does not exist.\")"
   ],
   "id": "d38d1f41e03f9bce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   title                                            summary  \\\n",
      "0  Dunning–Kruger effect  The Dunning–Kruger effect is a cognitive bias ...   \n",
      "\n",
      "                                        full_content  \n",
      "0  The Dunning–Kruger effect is a cognitive bias ...  \n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
